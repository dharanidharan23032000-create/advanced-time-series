{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Cell 1 – Install & Imports\n",
        "!pip install shap --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "\n",
        "import shap\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "import tensorflow as tf\n",
        "\n",
        "# Make sure everything runs in eager mode\n",
        "tf.config.run_functions_eagerly(True)\n",
        "print(\"Eager mode:\", tf.executing_eagerly())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuoen_UDdhX_",
        "outputId": "b0c85ad6-5e1b-4c41-ee0a-2c40e8e99360"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow: 2.19.0\n",
            "Eager mode: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 2 – Generate Multivariate Time Series\n",
        "def generate_synthetic_multivariate_series(n_steps: int = 2000) -> pd.DataFrame:\n",
        "    t = np.arange(n_steps)\n",
        "\n",
        "    base_trend = 0.01 * t\n",
        "    seasonal_1 = 2.0 * np.sin(2 * np.pi * t / 50)\n",
        "    seasonal_2 = 1.5 * np.sin(2 * np.pi * t / 30)\n",
        "    seasonal_3 = 1.0 * np.sin(2 * np.pi * t / 70)\n",
        "\n",
        "    noise1 = np.random.normal(scale=0.5, size=n_steps)\n",
        "    noise2 = np.random.normal(scale=0.3, size=n_steps)\n",
        "    noise3 = np.random.normal(scale=0.2, size=n_steps)\n",
        "    noise4 = np.random.normal(scale=0.7, size=n_steps)\n",
        "    noise5 = np.random.normal(scale=0.5, size=n_steps)\n",
        "\n",
        "    sensor_1 = base_trend + seasonal_1 + noise1\n",
        "    sensor_2 = 0.5 * base_trend + seasonal_2 + noise2\n",
        "    sensor_3 = -0.3 * base_trend + seasonal_3 + noise3\n",
        "    sensor_4 = 0.1 * base_trend + 0.5 * seasonal_1 + noise4\n",
        "    sensor_5 = 0.2 * base_trend - 0.3 * seasonal_2 + noise5\n",
        "\n",
        "    target = np.roll(sensor_1, -1)\n",
        "\n",
        "    data = {\n",
        "        \"sensor_1\": sensor_1[:-1],\n",
        "        \"sensor_2\": sensor_2[:-1],\n",
        "        \"sensor_3\": sensor_3[:-1],\n",
        "        \"sensor_4\": sensor_4[:-1],\n",
        "        \"sensor_5\": sensor_5[:-1],\n",
        "        \"target\":   target[:-1],\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "df = generate_synthetic_multivariate_series(2000)\n",
        "print(df.head())\n",
        "print(\"Shape:\", df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7mEKHAFdpCA",
        "outputId": "d4c5a7e2-b61f-4401-8253-84af9aada92e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sensor_1  sensor_2  sensor_3  sensor_4  sensor_5    target\n",
            "0  0.248357 -0.202553 -0.172699 -0.779857 -0.016513  0.191534\n",
            "1  0.191534  0.273512  0.080399 -0.315318 -0.343385  0.841224\n",
            "2  0.841224  0.382379  0.176160 -0.408752 -0.265219  1.527764\n",
            "3  1.527764  0.804289  0.351563 -0.012473  0.098863  0.886431\n",
            "4  0.886431  0.566633  0.066003  0.335848  0.312513  1.108502\n",
            "Shape: (1999, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 3 – Windowing & Metrics Helpers\n",
        "def create_windows(features, targets, seq_len=30, horizon=1):\n",
        "    X, y = [], []\n",
        "    total_steps = len(features)\n",
        "    for start in range(total_steps - seq_len - horizon + 1):\n",
        "        end = start + seq_len\n",
        "        X.append(features[start:end, :])\n",
        "        y.append(targets[end + horizon - 1])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def regression_metrics(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100.0\n",
        "    return mae, rmse, mape\n"
      ],
      "metadata": {
        "id": "8Yu8-j3WdyN6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 4 – Prepare Data (scale, split, windows)\n",
        "feature_cols = [\"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"]\n",
        "target_col = \"target\"\n",
        "\n",
        "features = df[feature_cols].values\n",
        "target = df[target_col].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "SEQ_LEN = 30\n",
        "HORIZON = 1\n",
        "\n",
        "X_all, y_all = create_windows(features_scaled, target, seq_len=SEQ_LEN, horizon=HORIZON)\n",
        "print(\"X_all:\", X_all.shape, \"y_all:\", y_all.shape)\n",
        "\n",
        "n_samples = len(X_all)\n",
        "train_end = int(0.7 * n_samples)\n",
        "val_end   = int(0.85 * n_samples)\n",
        "\n",
        "X_train, y_train = X_all[:train_end],      y_all[:train_end]\n",
        "X_val,   y_val   = X_all[train_end:val_end], y_all[train_end:val_end]\n",
        "X_test,  y_test  = X_all[val_end:],        y_all[val_end:]\n",
        "\n",
        "print(\"Train:\", X_train.shape)\n",
        "print(\"Val  :\", X_val.shape)\n",
        "print(\"Test :\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WXbB0cld2M6",
        "outputId": "77b726d1-6bb0-4161-bfce-106dc46d90e6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_all: (1969, 30, 5) y_all: (1969,)\n",
            "Train: (1378, 30, 5)\n",
            "Val  : (295, 30, 5)\n",
            "Test : (296, 30, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 5 – Build Model Function\n",
        "def build_lstm_model(input_shape,\n",
        "                     num_layers=2,\n",
        "                     hidden_size=64,\n",
        "                     learning_rate=1e-3):\n",
        "    # Clear any old graphs/models\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    model = models.Sequential(name=\"LSTM_Forecaster\")\n",
        "\n",
        "    # Explicit Input layer\n",
        "    model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        return_sequences = (i < num_layers - 1)\n",
        "        model.add(\n",
        "            layers.LSTM(\n",
        "                hidden_size,\n",
        "                return_sequences=return_sequences,\n",
        "                name=f\"lstm_{i+1}\",\n",
        "            )\n",
        "        )\n",
        "\n",
        "    model.add(layers.Dense(1, name=\"output\"))\n",
        "\n",
        "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    # ⭐ run_eagerly=True avoids the SymbolicTensor / Dataset issues\n",
        "    model.compile(\n",
        "        loss=\"mse\",\n",
        "        optimizer=optimizer,\n",
        "        metrics=[\"mae\"],\n",
        "        run_eagerly=True,\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "input_shape = (SEQ_LEN, X_all.shape[2])\n"
      ],
      "metadata": {
        "id": "AJv5dUuyd_p5"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we are in normal graph mode, not experimental eager mode\n",
        "param_grid = {\n",
        "    \"num_layers\":   [1, 2],\n",
        "    \"hidden_size\":  [32, 64],\n",
        "    \"learning_rate\":[1e-3, 3e-4],\n",
        "    \"batch_size\":   [32, 64],\n",
        "}\n",
        "\n",
        "best_config = None\n",
        "best_val_mae = np.inf\n",
        "best_model = None\n",
        "\n",
        "for nl in param_grid[\"num_layers\"]:\n",
        "    for hs in param_grid[\"hidden_size\"]:\n",
        "        for lr in param_grid[\"learning_rate\"]:\n",
        "            for bs in param_grid[\"batch_size\"]:\n",
        "\n",
        "                config = {\n",
        "                    \"num_layers\": nl,\n",
        "                    \"hidden_size\": hs,\n",
        "                    \"learning_rate\": lr,\n",
        "                    \"batch_size\": bs,\n",
        "                }\n",
        "                print(\"\\nConfig:\", config)\n",
        "\n",
        "                model = build_lstm_model(\n",
        "                    input_shape=input_shape,\n",
        "                    num_layers=nl,\n",
        "                    hidden_size=hs,\n",
        "                    learning_rate=lr,\n",
        "                )\n",
        "\n",
        "                es = callbacks.EarlyStopping(\n",
        "                    monitor=\"val_mae\",\n",
        "                    patience=5,\n",
        "                    restore_best_weights=True,\n",
        "                )\n",
        "\n",
        "                history = model.fit(\n",
        "                    X_train,\n",
        "                    y_train,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=50,\n",
        "                    batch_size=bs,\n",
        "                    callbacks=[es],\n",
        "                    verbose=0,\n",
        "                )\n",
        "\n",
        "                val_mae = float(np.min(history.history[\"val_mae\"]))\n",
        "                print(\"Validation MAE:\", val_mae)\n",
        "\n",
        "                if val_mae < best_val_mae:\n",
        "                    best_val_mae = val_mae\n",
        "                    best_config = config\n",
        "                    best_model = model\n",
        "\n",
        "print(\"\\nBest hyperparameters:\", best_config)\n",
        "print(\"Best validation MAE:\", best_val_mae)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "N_ViJvkaeBAb",
        "outputId": "1add9c7e-f591-40eb-f243-37675089832e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Config: {'num_layers': 1, 'hidden_size': 32, 'learning_rate': 0.001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "`tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-857705793.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                 )\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 history = model.fit(\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n\u001b[0m\u001b[1;32m    504\u001b[0m                          \"iteration in eager mode or within tf.function.\")\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: `tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 7 – Retrain Best Model on Train+Val and Evaluate\n",
        "# Merge train + val\n",
        "X_train_full = np.concatenate([X_train, X_val], axis=0)\n",
        "y_train_full = np.concatenate([y_train, y_val], axis=0)\n",
        "\n",
        "final_model = build_lstm_model(\n",
        "    input_shape,\n",
        "    num_layers=best_config[\"num_layers\"],\n",
        "    hidden_size=best_config[\"hidden_size\"],\n",
        "    learning_rate=best_config[\"learning_rate\"],\n",
        ")\n",
        "\n",
        "es_final = callbacks.EarlyStopping(\n",
        "    monitor=\"val_mae\",\n",
        "    patience=8,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history_final = final_model.fit(\n",
        "    X_train_full, y_train_full,\n",
        "    validation_split=0.1,\n",
        "    epochs=80,\n",
        "    batch_size=best_config[\"batch_size\"],\n",
        "    callbacks=[es_final],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "y_pred = final_model.predict(X_test).flatten()\n",
        "\n",
        "mae, rmse, mape = regression_metrics(y_test, y_pred)\n",
        "print(\"\\n===== Test Metrics =====\")\n",
        "print(\"MAE :\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"MAPE:\", mape)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(y_test, label=\"True\")\n",
        "plt.plot(y_pred, label=\"Predicted\")\n",
        "plt.title(\"LSTM Forecast: True vs Predicted (Test)\")\n",
        "plt.xlabel(\"Time index\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "VALkIrwpeG5A",
        "outputId": "1fe77480-84e2-4090-dd92-997c04184a4c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1343018584.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m final_model = build_lstm_model(\n\u001b[1;32m      7\u001b[0m     \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_layers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hidden_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 8 – SHAP Explainability\n",
        "# Use a small subset for SHAP (to keep it fast)\n",
        "background_size = min(200, len(X_train_full))\n",
        "explain_size = min(100, len(X_test))\n",
        "\n",
        "background = X_train_full[np.random.choice(len(X_train_full),\n",
        "                                           size=background_size,\n",
        "                                           replace=False)]\n",
        "explain_samples = X_test[:explain_size]\n",
        "\n",
        "explainer = shap.DeepExplainer(final_model, background)\n",
        "shap_values_list = explainer.shap_values(explain_samples)\n",
        "shap_values = shap_values_list[0]  # [samples, seq_len, num_features]\n",
        "\n",
        "feature_names = feature_cols\n",
        "\n",
        "# Global feature importance\n",
        "feature_importance = np.mean(np.abs(shap_values), axis=(0, 1))\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(range(len(feature_names)), feature_importance)\n",
        "plt.xticks(range(len(feature_names)), feature_names, rotation=45)\n",
        "plt.title(\"Global Feature Importance (mean |SHAP| over time)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Time-step importance\n",
        "timestep_importance = np.mean(np.abs(shap_values), axis=2)\n",
        "mean_timestep_importance = np.mean(timestep_importance, axis=0)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(1, SEQ_LEN + 1), mean_timestep_importance)\n",
        "plt.title(\"Average Time-Step Importance\")\n",
        "plt.xlabel(\"Time step in input window\")\n",
        "plt.ylabel(\"Mean |SHAP|\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "BL-SM_BTeUMl",
        "outputId": "b5733d2c-928a-4be2-eb0f-68d6dce3c3fd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'final_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3504681663.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mexplain_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mexplain_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeepExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mshap_values_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplain_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap_values_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# [samples, seq_len, num_features]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_W7Dwa_leXv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "hqlFWytlA7Kc",
        "outputId": "89a2b464-ad84-4b7d-ca2a-5a14f074b454"
      },
      "source": [
        "# Refactor the code to temporarily toggle eager execution on and off, allowing Keras model training to run with eager execution enabled and SHAP explainability to run with eager execution disabled, addressing the `RuntimeError`.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NOTE: All tf.compat.v1.disable_eager_execution() and tf.compat.v1.disable_v2_behavior()\n",
        "# calls are removed/commented out from the global scope and from inside main().\n",
        "# TensorFlow will run in its default eager mode (TF2.x behavior).\n",
        "# SHAP will be imported later and attempted to run in this eager environment.\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 1. DATA GENERATION – multivariate time series (5 features)\n",
        "# =========================================================\n",
        "\n",
        "def generate_synthetic_multivariate_series(n_steps: int = 2000) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Programmatically generate a multivariate time series that mimics\n",
        "    industrial sensor data with:\n",
        "      - clear trend\n",
        "      - multiple seasonalities\n",
        "      - Gaussian noise\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with 5 sensor features and a 1-step-ahead target.\n",
        "    \"\"\"\n",
        "    t = np.arange(n_steps)\n",
        "\n",
        "    # Global trend\n",
        "    base_trend = 0.01 * t\n",
        "\n",
        "    # Different seasonal components\n",
        "    seasonal_1 = 2.0 * np.sin(2 * np.pi * t / 50)   # period 50\n",
        "    seasonal_2 = 1.5 * np.sin(2 * np.pi * t / 30)   # period 30\n",
        "    seasonal_3 = 1.0 * np.sin(2 * np.pi * t / 70)   # period 70\n",
        "\n",
        "    # Sensor-level noise\n",
        "    noise1 = np.random.normal(scale=0.5, size=n_steps)\n",
        "    noise2 = np.random.normal(scale=0.3, size=n_steps)\n",
        "    noise3 = np.random.normal(scale=0.2, size=n_steps)\n",
        "    noise4 = np.random.normal(scale=0.7, size=n_steps)\n",
        "    noise5 = np.random.normal(scale=0.5, size=n_steps)\n",
        "\n",
        "    # Five correlated sensor signals\n",
        "    sensor_1 = base_trend + seasonal_1 + noise1\n",
        "    sensor_2 = 0.5 * base_trend + seasonal_2 + noise2\n",
        "    sensor_3 = -0.3 * base_trend + seasonal_3 + noise3\n",
        "    sensor_4 = 0.1 * base_trend + 0.5 * seasonal_1 + noise4\n",
        "    sensor_5 = 0.2 * base_trend - 0.3 * seasonal_2 + noise5\n",
        "\n",
        "    # Target is one-step-ahead of sensor_1 (forecasting)\n",
        "    target = np.roll(sensor_1, -1)\n",
        "\n",
        "    # Drop last position (roll artifact)\n",
        "    sensor_1 = sensor_1[:-1]\n",
        "    sensor_2 = sensor_2[:-1]\n",
        "    sensor_3 = sensor_3[:-1]\n",
        "    sensor_4 = sensor_4[:-1]\n",
        "    sensor_5 = sensor_5[:-1]\n",
        "    target = target[:-1]\n",
        "\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"sensor_1\": sensor_1,\n",
        "            \"sensor_2\": sensor_2,\n",
        "            \"sensor_3\": sensor_3,\n",
        "            \"sensor_4\": sensor_4,\n",
        "            \"sensor_5\": sensor_5,\n",
        "            \"target\": target,\n",
        "        }\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 2. SEQUENCE WINDOWING\n",
        "# =========================================================\n",
        "\n",
        "def create_windows(features: np.ndarray,\n",
        "                   targets: np.ndarray,\n",
        "                   seq_len: int = 30,\n",
        "                   horizon: int = 1):\n",
        "    \"\"\"\n",
        "    Convert multivariate time series into supervised learning windows.\n",
        "\n",
        "    Args:\n",
        "        features: array of shape [T, num_features]\n",
        "        targets: array of shape [T,]\n",
        "        seq_len: length of input sequence\n",
        "        horizon: forecasting horizon (steps ahead)\n",
        "\n",
        "    Returns:\n",
        "        X: [num_samples, seq_len, num_features]\n",
        "        y: [num_samples,]\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    total_steps = len(features)\n",
        "    for start in range(total_steps - seq_len - horizon + 1):\n",
        "        end = start + seq_len\n",
        "        X.append(features[start:end, :])\n",
        "        y.append(targets[end + horizon - 1])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 3. BUILD LSTM MODEL\n",
        "# =========================================================\n",
        "\n",
        "def build_lstm_model(input_shape,\n",
        "                     num_layers: int = 2,\n",
        "                     hidden_size: int = 64,\n",
        "                     learning_rate: float = 1e-3) -> tf.keras.Model:\n",
        "    \"\"\"\n",
        "    Build and compile an LSTM model for time series forecasting.\n",
        "\n",
        "    Args:\n",
        "        input_shape: (seq_len, num_features)\n",
        "        num_layers: number of LSTM layers\n",
        "        hidden_size: hidden units per layer\n",
        "        learning_rate: learning rate for Adam optimizer\n",
        "    \"\"\"\n",
        "    model = models.Sequential(name=\"LSTM_Forecaster\")\n",
        "    for i in range(num_layers):\n",
        "        return_sequences = i < (num_layers - 1)\n",
        "        if i == 0:\n",
        "            model.add(\n",
        "                layers.LSTM(\n",
        "                    hidden_size,\n",
        "                    return_sequences=return_sequences,\n",
        "                    input_shape=input_shape,\n",
        "                    name=f\"lstm_{i+1}\",\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            model.add(\n",
        "                layers.LSTM(\n",
        "                    hidden_size,\n",
        "                    return_sequences=return_sequences,\n",
        "                    name=f\"lstm_{i+1}\",\n",
        "                )\n",
        "            )\n",
        "\n",
        "    model.add(layers.Dense(1, name=\"output\"))\n",
        "\n",
        "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mae\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 4. METRICS\n",
        "# =========================================================\n",
        "\n",
        "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "    \"\"\"\n",
        "    Compute MAE, RMSE, and MAPE for regression.\n",
        "    \"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100.0\n",
        "    return mae, rmse, mape\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 5. FULL PIPELINE\n",
        "# =========================================================\n",
        "\n",
        "def main():\n",
        "    # -----------------------------------------------------\n",
        "    # 5.1 Dataset creation\n",
        "    # -----------------------------------------------------\n",
        "    df = generate_synthetic_multivariate_series(n_steps=2000)\n",
        "    print(\"Dataset head:\\n\", df.head())\n",
        "    print(\"Dataset shape:\", df.shape)\n",
        "\n",
        "    feature_cols = [\"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"]\n",
        "    target_col = \"target\"\n",
        "\n",
        "    features = df[feature_cols].values\n",
        "    target = df[target_col].values\n",
        "\n",
        "    # Standardize features (target kept in original scale)\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "    seq_len = 30\n",
        "    horizon = 1\n",
        "\n",
        "    X_all, y_all = create_windows(\n",
        "        features_scaled,\n",
        "        target,\n",
        "        seq_len=seq_len,\n",
        "        horizon=horizon,\n",
        "    )\n",
        "    print(\"X_all shape:\", X_all.shape)  # (samples, seq_len, num_features)\n",
        "    print(\"y_all shape:\", y_all.shape)\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # 5.2 Time-based train / val / test split\n",
        "    # -----------------------------------------------------\n",
        "    n_samples = len(X_all)\n",
        "    train_end = int(0.7 * n_samples)\n",
        "    val_end = int(0.85 * n_samples)\n",
        "\n",
        "    X_train, y_train = X_all[:train_end], y_all[:train_end]\n",
        "    X_val, y_val = X_all[train_end:val_end], y_all[train_end:val_end]\n",
        "    X_test, y_test = X_all[val_end:], y_all[val_end:]\n",
        "\n",
        "    print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "    input_shape = (seq_len, X_all.shape[2])\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # 5.3 Hyperparameter tuning – small grid search\n",
        "    # -----------------------------------------------------\n",
        "    param_grid = {\n",
        "        \"num_layers\": [1, 2],\n",
        "        \"hidden_size\": [32, 64],\n",
        "        \"learning_rate\": [1e-3, 3e-4],\n",
        "        \"batch_size\": [32, 64],\n",
        "    }\n",
        "\n",
        "    best_config = None\n",
        "    best_val_mae = np.inf\n",
        "    best_model = None\n",
        "\n",
        "    for num_layers in param_grid[\"num_layers\"]:\n",
        "        for hidden_size in param_grid[\"hidden_size\"]:\n",
        "            for lr in param_grid[\"learning_rate\"]:\n",
        "                for batch_size in param_grid[\"batch_size\"]:\n",
        "                    config = {\n",
        "                        \"num_layers\": num_layers,\n",
        "                        \"hidden_size\": hidden_size,\n",
        "                        \"learning_rate\": lr,\n",
        "                        \"batch_size\": batch_size,\n",
        "                    }\n",
        "                    print(\"\\nTraining configuration:\", config)\n",
        "\n",
        "                    model = build_lstm_model(\n",
        "                        input_shape=input_shape,\n",
        "                        num_layers=num_layers,\n",
        "                        hidden_size=hidden_size,\n",
        "                        learning_rate=lr,\n",
        "                    )\n",
        "\n",
        "                    es = callbacks.EarlyStopping(\n",
        "                        monitor=\"val_mae\",\n",
        "                        patience=5,\n",
        "                        restore_best_weights=True,\n",
        "                    )\n",
        "\n",
        "                    history = model.fit(\n",
        "                        X_train,\n",
        "                        y_train,\n",
        "                        epochs=50,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        callbacks=[es],\n",
        "                        verbose=0,\n",
        "                    )\n",
        "\n",
        "                    val_mae = float(np.min(history.history[\"val_mae\"]))\n",
        "                    print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "                    if val_mae < best_val_mae:\n",
        "                        best_val_mae = val_mae\n",
        "                        best_config = config\n",
        "                        best_model = model\n",
        "\n",
        "    print(\"\\nBest hyperparameters:\", best_config)\n",
        "    print(\"Best validation MAE:\", best_val_mae)\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # 5.4 Retrain best model on Train + Val\n",
        "    # -----------------------------------------------------\n",
        "    X_train_full = np.concatenate([X_train, X_val], axis=0)\n",
        "    y_train_full = np.concatenate([y_train, y_val], axis=0)\n",
        "\n",
        "    final_model = build_lstm_model(\n",
        "        input_shape=input_shape,\n",
        "        num_layers=best_config[\"num_layers\"],\n",
        "        hidden_size=best_config[\"hidden_size\"],\n",
        "        learning_rate=best_config[\"learning_rate\"],\n",
        "    )\n",
        "\n",
        "    es_final = callbacks.EarlyStopping(\n",
        "        monitor=\"val_mae\",\n",
        "        patience=8,\n",
        "        restore_best_weights=True,\n",
        "    )\n",
        "\n",
        "    history_final = final_model.fit(\n",
        "        X_train_full,\n",
        "        y_train_full,\n",
        "        epochs=80,\n",
        "        batch_size=best_config[\"batch_size\"],\n",
        "        validation_split=0.1,\n",
        "        callbacks=[es_final],\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # 5.5 Evaluation on test set\n",
        "    # -----------------------------------------------------\n",
        "    y_pred = final_model.predict(X_test).flatten()\n",
        "\n",
        "    mae, rmse, mape = regression_metrics(y_test, y_pred)\n",
        "    print(\"\\n===== Test Metrics =====\")\n",
        "    print(f\"MAE : {mae:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAPE: {mape:.2f}%\")\n",
        "\n",
        "    # Plot true vs predicted\n",
        "    plt.figure()\n",
        "    plt.plot(y_test, label=\"True\")\n",
        "    plt.plot(y_pred, label=\"Predicted\")\n",
        "    plt.title(\"LSTM Forecast: True vs Predicted (Test Set)\")\n",
        "    plt.xlabel(\"Time index\")\n",
        "    plt.ylabel(\"Target\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # 5.6 Explainability with SHAP\n",
        "    # -----------------------------------------------------\n",
        "    # Import SHAP right before its usage to minimize potential TensorFlow eager conflicts\n",
        "    import shap\n",
        "\n",
        "    # SHAP DeepExplainer often requires graph mode (TensorFlow 1.x behavior).\n",
        "    # Attempting to run SHAP in TF2.x eager mode. If this fails, then more advanced\n",
        "    # techniques like a kernel restart or alternative explainability methods are needed.\n",
        "\n",
        "    background_size = min(200, len(X_train_full))\n",
        "    explain_size = min(200, len(X_test))\n",
        "\n",
        "    background = X_train_full[\n",
        "        np.random.choice(len(X_train_full), size=background_size, replace=False)\n",
        "    ]\n",
        "    explain_samples = X_test[:explain_size]\n",
        "\n",
        "    # DeepExplainer for Keras model\n",
        "    explainer = shap.DeepExplainer(final_model, background)\n",
        "    shap_values_list = explainer.shap_values(explain_samples)\n",
        "    # For regression models shap_values is a list with one element\n",
        "    shap_values = shap_values_list[0]  # [samples, seq_len, num_features]\n",
        "\n",
        "    # ---- Global feature importance over all time steps ----\n",
        "    feature_importance = np.mean(np.abs(shap_values), axis=(0, 1))\n",
        "    feature_names = feature_cols\n",
        "\n",
        "    plt.figure()\n",
        "    plt.bar(range(len(feature_names)), feature_importance)\n",
        "    plt.xticks(range(len(feature_names)), feature_names, rotation=45)\n",
        "    plt.title(\"Global Feature Importance (mean |SHAP| over time)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ---- Time-step importance (which positions in sequence matter) ----\n",
        "    timestep_importance = np.mean(np.abs(shap_values), axis=2)  # [samples, seq_len]\n",
        "    mean_timestep_importance = np.mean(timestep_importance, axis=0)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, seq_len + 1), mean_timestep_importance)\n",
        "    plt.title(\"Average Time-Step Importance (mean |SHAP| over features)\")\n",
        "    plt.xlabel(\"Time step in input window\")\n",
        "    plt.ylabel(\"Importance\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Optional SHAP summary plot (commented because it needs proper backend)\n",
        "    # shap.summary_plot(\n",
        "    #     shap_values.reshape(-1, shap_values.shape[-1]),\n",
        "    #     explain_samples.reshape(-1, explain_samples.shape[-1]),\n",
        "    #     feature_names=feature_names * seq_len,\n",
        "    # )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset head:\n",
            "    sensor_1  sensor_2  sensor_3  sensor_4  sensor_5    target\n",
            "0  0.248357 -0.202553 -0.172699 -0.779857 -0.016513  0.191534\n",
            "1  0.191534  0.273512  0.080399 -0.315318 -0.343385  0.841224\n",
            "2  0.841224  0.382379  0.176160 -0.408752 -0.265219  1.527764\n",
            "3  1.527764  0.804289  0.351563 -0.012473  0.098863  0.886431\n",
            "4  0.886431  0.566633  0.066003  0.335848  0.312513  1.108502\n",
            "Dataset shape: (1999, 6)\n",
            "X_all shape: (1969, 30, 5)\n",
            "y_all shape: (1969,)\n",
            "Train: (1378, 30, 5), Val: (295, 30, 5), Test: (296, 30, 5)\n",
            "\n",
            "Training configuration: {'num_layers': 1, 'hidden_size': 32, 'learning_rate': 0.001, 'batch_size': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py:2341: UserWarning: Seed 42 from outer graph might be getting used by function Dataset_map_permutation, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.\n",
            "  return map_op._map_v2(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "`tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3262575730.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3262575730.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    261\u001b[0m                     )\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                     history = model.fit(\n\u001b[0m\u001b[1;32m    264\u001b[0m                         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n\u001b[0m\u001b[1;32m    504\u001b[0m                          \"iteration in eager mode or within tf.function.\")\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: `tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function."
          ]
        }
      ]
    }
  ]
}